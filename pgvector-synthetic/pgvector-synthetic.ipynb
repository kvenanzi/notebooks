{"cells": [{"cell_type": "markdown", "id": "dbbd84d6-fc7b-44bd-95bc-e3d9278496bd", "metadata": {}, "source": ["# Prerequisites\n", "* PostgreSQL reachable at localhost:5432 with a database named patient_db and credentials matching the connection cell (user=\"kevin\", password=\"password123\"). Install the pgvector extension in this database.\n", "* Python environment capable of installing packages listed below (the notebook relies on pip inside the runtime).\n", "* Data files ./patients.csv and ./allergies.csv present relative to the notebook.\n", "* Ollama running locally on the default port 11434 with the phi4-mini embedding model pulled and ready (ollama pull phi4-mini)."]}, {"cell_type": "markdown", "id": "e6d45dac-2f8f-443a-a931-adb006c70589", "metadata": {}, "source": ["# 1. Install dependencies"]}, {"cell_type": "code", "execution_count": 1, "id": "5528b6a6-de86-4902-b6d1-e9a48ce921f2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Collecting psycopg2-binary\n", "  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n", "Collecting pandas\n", "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n", "Collecting numpy\n", "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n", "Collecting matplotlib\n", "  Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n", "Collecting plotly\n", "  Downloading plotly-6.4.0-py3-none-any.whl.metadata (8.5 kB)\n", "Collecting faker\n", "  Downloading faker-37.12.0-py3-none-any.whl.metadata (15 kB)\n", "Requirement already satisfied: requests in /home/kevin/miniforge3/lib/python3.12/site-packages (2.32.5)\n", "Collecting umap-learn\n", "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n", "Collecting tabulate\n", "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n", "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kevin/miniforge3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n", "Requirement already satisfied: pytz>=2020.1 in /home/kevin/miniforge3/lib/python3.12/site-packages (from pandas) (2025.2)\n", "Requirement already satisfied: tzdata>=2022.7 in /home/kevin/miniforge3/lib/python3.12/site-packages (from pandas) (2025.2)\n", "Collecting contourpy>=1.0.1 (from matplotlib)\n", "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n", "Collecting cycler>=0.10 (from matplotlib)\n", "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n", "Collecting fonttools>=4.22.0 (from matplotlib)\n", "  Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n", "Collecting kiwisolver>=1.3.1 (from matplotlib)\n", "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n", "Requirement already satisfied: packaging>=20.0 in /home/kevin/miniforge3/lib/python3.12/site-packages (from matplotlib) (25.0)\n", "Collecting pillow>=8 (from matplotlib)\n", "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n", "Collecting pyparsing>=3 (from matplotlib)\n", "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n", "Collecting narwhals>=1.15.1 (from plotly)\n", "  Downloading narwhals-2.10.2-py3-none-any.whl.metadata (11 kB)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /home/kevin/miniforge3/lib/python3.12/site-packages (from requests) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /home/kevin/miniforge3/lib/python3.12/site-packages (from requests) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kevin/miniforge3/lib/python3.12/site-packages (from requests) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /home/kevin/miniforge3/lib/python3.12/site-packages (from requests) (2025.10.5)\n", "Collecting scipy>=1.3.1 (from umap-learn)\n", "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n", "Collecting scikit-learn>=1.6 (from umap-learn)\n", "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n", "Collecting numba>=0.51.2 (from umap-learn)\n", "  Downloading numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n", "Collecting pynndescent>=0.5 (from umap-learn)\n", "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n", "Requirement already satisfied: tqdm in /home/kevin/miniforge3/lib/python3.12/site-packages (from umap-learn) (4.67.1)\n", "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.2->umap-learn)\n", "  Downloading llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n", "Collecting joblib>=0.11 (from pynndescent>=0.5->umap-learn)\n", "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n", "Requirement already satisfied: six>=1.5 in /home/kevin/miniforge3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n", "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.6->umap-learn)\n", "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n", "Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n", "\u001b[?25hUsing cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n", "Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n", "\u001b[?25hUsing cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n", "Downloading plotly-6.4.0-py3-none-any.whl (9.9 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n", "\u001b[?25hDownloading faker-37.12.0-py3-none-any.whl (2.0 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n", "\u001b[?25hDownloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n", "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n", "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n", "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n", "Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n", "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n", "Downloading narwhals-2.10.2-py3-none-any.whl (419 kB)\n", "Downloading numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n", "\u001b[?25hDownloading llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n", "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n", "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n", "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n", "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n", "Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n", "\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n", "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n", "Installing collected packages: threadpoolctl, tabulate, pyparsing, psycopg2-binary, pillow, numpy, narwhals, llvmlite, kiwisolver, joblib, fonttools, faker, cycler, scipy, plotly, pandas, numba, contourpy, scikit-learn, matplotlib, pynndescent, umap-learn\n", "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22/22\u001b[0m [umap-learn]0m \u001b[32m21/22\u001b[0m [umap-learn]lotlib]n]\n", "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 faker-37.12.0 fonttools-4.60.1 joblib-1.5.2 kiwisolver-1.4.9 llvmlite-0.45.1 matplotlib-3.10.7 narwhals-2.10.2 numba-0.62.1 numpy-2.3.4 pandas-2.3.3 pillow-12.0.0 plotly-6.4.0 psycopg2-binary-2.9.11 pynndescent-0.5.13 pyparsing-3.2.5 scikit-learn-1.7.2 scipy-1.16.3 tabulate-0.9.0 threadpoolctl-3.6.0 umap-learn-0.5.9.post2\n"]}], "source": "!pip install psycopg2-binary pandas numpy matplotlib plotly faker requests umap-learn tabulate python-dotenv\n"}, {"cell_type": "markdown", "id": "6ee55fbb-9abf-4190-a296-8f90d6a26fc3", "metadata": {}, "source": ["# 2. Imports"]}, {"cell_type": "code", "execution_count": 2, "id": "c8fce2b4-318b-4ce7-a855-9adc383a7d8b", "metadata": {}, "outputs": [], "source": ["from dotenv import load_dotenv\n", "import os, requests\n", "import psycopg2\n", "import pandas as pd\n", "import numpy as np\n", "from io import StringIO\n", "import plotly.express as px\n", "load_dotenv()\n"]}, {"cell_type": "markdown", "id": "4f5d7c53-03dc-47b6-9ebf-35e319b6645d", "metadata": {}, "source": ["# 3. Connect to PostgreSQL with pgvector"]}, {"cell_type": "code", "execution_count": 3, "id": "fbcbfbf6-db0e-4722-bc5a-e09f15589265", "metadata": {}, "outputs": [], "source": ["conn = psycopg2.connect(\n", "    dbname=\"patient_db\",\n", "    user=\"kevin\",\n", "    password=\"password123\",\n", "    host=\"localhost\",\n", "    port=6432\n", ")\n", "cur = conn.cursor()\n", "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n", "conn.commit()\n"]}, {"cell_type": "markdown", "id": "67033158-c8bd-4347-86b8-aabf3698b69d", "metadata": {}, "source": ["# 4. Load CSVs"]}, {"cell_type": "code", "execution_count": 4, "id": "463260e5-c89a-4e34-99e4-822b7a6872f0", "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Patients:\n"]}, {"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>patient_id</th>\n", "      <th>vista_id</th>\n", "      <th>mrn</th>\n", "      <th>first_name</th>\n", "      <th>last_name</th>\n", "      <th>middle_name</th>\n", "      <th>gender</th>\n", "      <th>birthdate</th>\n", "      <th>age</th>\n", "      <th>race</th>\n", "      <th>...</th>\n", "      <th>genetic_markers</th>\n", "      <th>precision_markers</th>\n", "      <th>comorbidity_profile</th>\n", "      <th>care_plan_total</th>\n", "      <th>care_plan_completed</th>\n", "      <th>care_plan_overdue</th>\n", "      <th>care_plan_scheduled</th>\n", "      <th>deceased</th>\n", "      <th>death_date</th>\n", "      <th>death_primary_cause</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>a7896ff7-3cfb-4a4f-ae92-02f58d2d691e</td>\n", "      <td>1168769</td>\n", "      <td>MRN893749</td>\n", "      <td>Kari</td>\n", "      <td>Hutchinson</td>\n", "      <td>D</td>\n", "      <td>female</td>\n", "      <td>2018-11-01</td>\n", "      <td>7</td>\n", "      <td>White</td>\n", "      <td>...</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>False</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>c89b6067-c181-412f-be7f-5ee597961ad7</td>\n", "      <td>9206201</td>\n", "      <td>MRN955810</td>\n", "      <td>Joseph</td>\n", "      <td>Swanson</td>\n", "      <td>W</td>\n", "      <td>male</td>\n", "      <td>1935-11-22</td>\n", "      <td>90</td>\n", "      <td>White</td>\n", "      <td>...</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>3</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>0</td>\n", "      <td>True</td>\n", "      <td>2025-10-30</td>\n", "      <td>Acute myocardial infarction, unspecified</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>748503cd-2632-4e29-8c06-558ea9d64548</td>\n", "      <td>8857864</td>\n", "      <td>MRN854471</td>\n", "      <td>Mark</td>\n", "      <td>Henderson</td>\n", "      <td>B</td>\n", "      <td>other</td>\n", "      <td>1989-11-08</td>\n", "      <td>36</td>\n", "      <td>White</td>\n", "      <td>...</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>False</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>7701558f-0d71-492b-ae3c-e9e93aa79b86</td>\n", "      <td>6727969</td>\n", "      <td>MRN501764</td>\n", "      <td>Tina</td>\n", "      <td>Castillo</td>\n", "      <td>A</td>\n", "      <td>male</td>\n", "      <td>1965-11-14</td>\n", "      <td>60</td>\n", "      <td>White</td>\n", "      <td>...</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>[{\"primary\": \"Hypertension\", \"associated\": \"Di...</td>\n", "      <td>6</td>\n", "      <td>1</td>\n", "      <td>5</td>\n", "      <td>0</td>\n", "      <td>True</td>\n", "      <td>2021-10-31</td>\n", "      <td>Intentional self-harm by unspecified means, in...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>3d0a4feb-f79e-4624-9c33-c2eb1c90489e</td>\n", "      <td>9256242</td>\n", "      <td>MRN248802</td>\n", "      <td>Jason</td>\n", "      <td>Smith</td>\n", "      <td>K</td>\n", "      <td>male</td>\n", "      <td>2001-11-05</td>\n", "      <td>24</td>\n", "      <td>Asian</td>\n", "      <td>...</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>[]</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>False</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>5 rows √ó 47 columns</p>\n", "</div>"], "text/plain": ["                             patient_id  vista_id        mrn first_name  \\\n", "0  a7896ff7-3cfb-4a4f-ae92-02f58d2d691e   1168769  MRN893749       Kari   \n", "1  c89b6067-c181-412f-be7f-5ee597961ad7   9206201  MRN955810     Joseph   \n", "2  748503cd-2632-4e29-8c06-558ea9d64548   8857864  MRN854471       Mark   \n", "3  7701558f-0d71-492b-ae3c-e9e93aa79b86   6727969  MRN501764       Tina   \n", "4  3d0a4feb-f79e-4624-9c33-c2eb1c90489e   9256242  MRN248802      Jason   \n", "\n", "    last_name middle_name  gender   birthdate  age   race  ...  \\\n", "0  Hutchinson           D  female  2018-11-01    7  White  ...   \n", "1     Swanson           W    male  1935-11-22   90  White  ...   \n", "2   Henderson           B   other  1989-11-08   36  White  ...   \n", "3    Castillo           A    male  1965-11-14   60  White  ...   \n", "4       Smith           K    male  2001-11-05   24  Asian  ...   \n", "\n", "  genetic_markers precision_markers  \\\n", "0              []                []   \n", "1              []                []   \n", "2              []                []   \n", "3              []                []   \n", "4              []                []   \n", "\n", "                                 comorbidity_profile care_plan_total  \\\n", "0                                                 []               0   \n", "1                                                 []               3   \n", "2                                                 []               0   \n", "3  [{\"primary\": \"Hypertension\", \"associated\": \"Di...               6   \n", "4                                                 []               0   \n", "\n", "   care_plan_completed care_plan_overdue care_plan_scheduled deceased  \\\n", "0                    0                 0                   0    False   \n", "1                    0                 3                   0     True   \n", "2                    0                 0                   0    False   \n", "3                    1                 5                   0     True   \n", "4                    0                 0                   0    False   \n", "\n", "   death_date                                death_primary_cause  \n", "0         NaN                                                NaN  \n", "1  2025-10-30           Acute myocardial infarction, unspecified  \n", "2         NaN                                                NaN  \n", "3  2021-10-31  Intentional self-harm by unspecified means, in...  \n", "4         NaN                                                NaN  \n", "\n", "[5 rows x 47 columns]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Allergies:\n"]}, {"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>allergy_id</th>\n", "      <th>patient_id</th>\n", "      <th>substance</th>\n", "      <th>category</th>\n", "      <th>reaction</th>\n", "      <th>reaction_code</th>\n", "      <th>reaction_system</th>\n", "      <th>severity</th>\n", "      <th>severity_code</th>\n", "      <th>severity_system</th>\n", "      <th>rxnorm_code</th>\n", "      <th>unii_code</th>\n", "      <th>snomed_code</th>\n", "      <th>risk_level</th>\n", "      <th>registry_source</th>\n", "      <th>recorded_date</th>\n", "      <th>followup_summary</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>d8141746-f50b-4636-ba63-e7df88496831</td>\n", "      <td>a7896ff7-3cfb-4a4f-ae92-02f58d2d691e</td>\n", "      <td>Acremonium Strictum 50 Mg/Ml Injectable Solution</td>\n", "      <td>environment</td>\n", "      <td>Angioedema</td>\n", "      <td>41291007</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>mild</td>\n", "      <td>255604002</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>905073</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>standard</td>\n", "      <td>warehouse</td>\n", "      <td>2021-04-22</td>\n", "      <td>risk: standard | severity: mild</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>292baf74-fac6-4862-b3c8-720d1460ea3c</td>\n", "      <td>c89b6067-c181-412f-be7f-5ee597961ad7</td>\n", "      <td>Administration Of First Dose Of Vaccine Produc...</td>\n", "      <td>drug</td>\n", "      <td>Nausea</td>\n", "      <td>422587007</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>mild</td>\n", "      <td>255604002</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>416591003</td>\n", "      <td>NaN</td>\n", "      <td>416591003.0</td>\n", "      <td>standard</td>\n", "      <td>warehouse</td>\n", "      <td>1956-12-14</td>\n", "      <td>risk: standard | severity: mild</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>5cf42c32-c2ed-451c-ad42-da9c600a001f</td>\n", "      <td>c89b6067-c181-412f-be7f-5ee597961ad7</td>\n", "      <td>Buckwheat 100 Mg/Ml Injectable Solution</td>\n", "      <td>food</td>\n", "      <td>Nausea</td>\n", "      <td>422587007</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>mild</td>\n", "      <td>255604002</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>904800</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>standard</td>\n", "      <td>warehouse</td>\n", "      <td>1969-09-19</td>\n", "      <td>risk: standard | severity: mild</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>52d3a0b5-5a55-4a54-8dd2-4589ca069618</td>\n", "      <td>748503cd-2632-4e29-8c06-558ea9d64548</td>\n", "      <td>Banana 100 Mg/Ml Injectable Solution</td>\n", "      <td>food</td>\n", "      <td>Urticaria</td>\n", "      <td>126485001</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>mild</td>\n", "      <td>255604002</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>891833</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>standard</td>\n", "      <td>warehouse</td>\n", "      <td>1995-11-08</td>\n", "      <td>risk: standard | severity: mild</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>3a3027b9-8477-447b-971e-6dbd3fbdac0c</td>\n", "      <td>748503cd-2632-4e29-8c06-558ea9d64548</td>\n", "      <td>Bee Venom</td>\n", "      <td>insect</td>\n", "      <td>Wheezing</td>\n", "      <td>56018004</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>moderate</td>\n", "      <td>6736007</td>\n", "      <td>http://snomed.info/sct</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>high</td>\n", "      <td>curated</td>\n", "      <td>1993-12-11</td>\n", "      <td>risk: high | severity: moderate</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                             allergy_id                            patient_id  \\\n", "0  d8141746-f50b-4636-ba63-e7df88496831  a7896ff7-3cfb-4a4f-ae92-02f58d2d691e   \n", "1  292baf74-fac6-4862-b3c8-720d1460ea3c  c89b6067-c181-412f-be7f-5ee597961ad7   \n", "2  5cf42c32-c2ed-451c-ad42-da9c600a001f  c89b6067-c181-412f-be7f-5ee597961ad7   \n", "3  52d3a0b5-5a55-4a54-8dd2-4589ca069618  748503cd-2632-4e29-8c06-558ea9d64548   \n", "4  3a3027b9-8477-447b-971e-6dbd3fbdac0c  748503cd-2632-4e29-8c06-558ea9d64548   \n", "\n", "                                           substance     category    reaction  \\\n", "0   Acremonium Strictum 50 Mg/Ml Injectable Solution  environment  Angioedema   \n", "1  Administration Of First Dose Of Vaccine Produc...         drug      Nausea   \n", "2            Buckwheat 100 Mg/Ml Injectable Solution         food      Nausea   \n", "3               Banana 100 Mg/Ml Injectable Solution         food   Urticaria   \n", "4                                          Bee Venom       insect    Wheezing   \n", "\n", "   reaction_code         reaction_system  severity  severity_code  \\\n", "0       41291007  http://snomed.info/sct      mild      255604002   \n", "1      422587007  http://snomed.info/sct      mild      255604002   \n", "2      422587007  http://snomed.info/sct      mild      255604002   \n", "3      126485001  http://snomed.info/sct      mild      255604002   \n", "4       56018004  http://snomed.info/sct  moderate        6736007   \n", "\n", "          severity_system rxnorm_code unii_code  snomed_code risk_level  \\\n", "0  http://snomed.info/sct      905073       NaN          NaN   standard   \n", "1  http://snomed.info/sct   416591003       NaN  416591003.0   standard   \n", "2  http://snomed.info/sct      904800       NaN          NaN   standard   \n", "3  http://snomed.info/sct      891833       NaN          NaN   standard   \n", "4  http://snomed.info/sct         NaN       NaN          NaN       high   \n", "\n", "  registry_source recorded_date                 followup_summary  \n", "0       warehouse    2021-04-22  risk: standard | severity: mild  \n", "1       warehouse    1956-12-14  risk: standard | severity: mild  \n", "2       warehouse    1969-09-19  risk: standard | severity: mild  \n", "3       warehouse    1995-11-08  risk: standard | severity: mild  \n", "4         curated    1993-12-11  risk: high | severity: moderate  "]}, "metadata": {}, "output_type": "display_data"}], "source": ["patients_df = pd.read_csv(\"./data/patients.csv\")\n", "allergies_df = pd.read_csv(\"./data/allergies.csv\")\n", "\n", "print(\"Patients:\")\n", "display(patients_df.head())\n", "\n", "print(\"Allergies:\")\n", "display(allergies_df.head())\n"]}, {"cell_type": "markdown", "id": "8d2cef44-5ff5-4bb0-90da-deb89853c1bd", "metadata": {}, "source": ["# 5. Create normalized tables"]}, {"cell_type": "code", "execution_count": 5, "id": "20f21eae-a466-493e-a39b-56523b962caf", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["‚úÖ Tables recreated successfully\n"]}], "source": ["conn.rollback()\n", "\n", "cur.execute(\"\"\"\n", "DROP TABLE IF EXISTS allergies CASCADE;\n", "DROP TABLE IF EXISTS patients CASCADE;\n", "\n", "CREATE TABLE patients (\n", "    patient_id TEXT PRIMARY KEY,\n", "    vista_id TEXT,\n", "    mrn TEXT,\n", "    first_name TEXT,\n", "    last_name TEXT,\n", "    middle_name TEXT,\n", "    gender TEXT,\n", "    birthdate TEXT,          -- changed from DATE to TEXT for flexibility\n", "    age INT,\n", "    race TEXT,\n", "    ethnicity TEXT,\n", "    address TEXT,\n", "    city TEXT,\n", "    state TEXT,\n", "    zip TEXT,\n", "    country TEXT,\n", "    phone TEXT,\n", "    email TEXT,\n", "    marital_status TEXT,\n", "    language TEXT,\n", "    insurance TEXT,\n", "    ssn TEXT,\n", "    smoking_status TEXT,\n", "    alcohol_use TEXT,\n", "    education TEXT,\n", "    employment_status TEXT,\n", "    income TEXT,\n", "    housing_status TEXT,\n", "    sdoh_risk_score FLOAT,\n", "    sdoh_risk_factors TEXT,\n", "    community_deprivation_index FLOAT,\n", "    access_to_care_score FLOAT,\n", "    transportation_access TEXT,\n", "    language_access_barrier TEXT,\n", "    social_support_score FLOAT,\n", "    sdoh_care_gaps TEXT,\n", "    genetic_risk_score FLOAT,\n", "    genetic_markers TEXT,\n", "    precision_markers TEXT,\n", "    comorbidity_profile TEXT,\n", "    care_plan_total INT,\n", "    care_plan_completed INT,\n", "    care_plan_overdue INT,\n", "    care_plan_scheduled INT,\n", "    deceased BOOLEAN,\n", "    death_date TEXT,         -- also TEXT now\n", "    death_primary_cause TEXT\n", ");\n", "\n", "CREATE TABLE allergies (\n", "    allergy_id TEXT PRIMARY KEY,\n", "    patient_id TEXT,\n", "    substance TEXT,\n", "    category TEXT,\n", "    reaction TEXT,\n", "    reaction_code TEXT,\n", "    reaction_system TEXT,\n", "    severity TEXT,\n", "    severity_code TEXT,\n", "    severity_system TEXT,\n", "    rxnorm_code TEXT,\n", "    unii_code TEXT,\n", "    snomed_code TEXT,\n", "    risk_level TEXT,\n", "    registry_source TEXT,\n", "    recorded_date TEXT,\n", "    followup_summary TEXT\n", ");\n", "\"\"\")\n", "conn.commit()\n", "print(\"‚úÖ Tables recreated successfully\")\n"]}, {"cell_type": "markdown", "id": "f4290b02-8a37-4d3c-9ebe-83d0ea6759b5", "metadata": {}, "source": ["# 6. Bulk-insert dataframes into Postgres"]}, {"cell_type": "code", "execution_count": 6, "id": "da2af22f-9ccc-4f71-9f4e-59db3803307d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Patients: 1000 | Allergies (filtered): 1026\n", "‚úÖ Loaded 1000 rows into patients\n", "‚úÖ Loaded 1026 rows into allergies\n"]}], "source": ["from io import StringIO\n", "\n", "def copy_dataframe(df, table_name):\n", "    # Make sure nulls are proper SQL NULLs\n", "    df = df.replace({np.nan: None})\n", "\n", "    # Write DataFrame to CSV buffer with quoting handled by pandas\n", "    buffer = StringIO()\n", "    df.to_csv(\n", "        buffer,\n", "        index=False,\n", "        header=False,\n", "        sep=\",\",\n", "        quoting=1,  # csv.QUOTE_ALL\n", "        escapechar=\"\\\\\"\n", "    )\n", "    buffer.seek(0)\n", "\n", "    try:\n", "        cur.copy_expert(\n", "            sql=f\"COPY {table_name} FROM STDIN WITH (FORMAT CSV, HEADER FALSE, DELIMITER ',', QUOTE '\\\"', ESCAPE '\\\\')\",\n", "            file=buffer\n", "        )\n", "        conn.commit()\n", "        print(f\"‚úÖ Loaded {len(df)} rows into {table_name}\")\n", "    except Exception as e:\n", "        conn.rollback()\n", "        print(f\"‚ùå Error loading {table_name}: {e}\")\n", "\n", "# --- Clean and normalize the patients dataframe ---\n", "patients_df = patients_df.replace({np.nan: None})  # Convert NaN to None\n", "\n", "# Normalize date columns: replace blanks with None, cast to str for CSV writing\n", "for col in [\"birthdate\", \"death_date\"]:\n", "    patients_df[col] = patients_df[col].apply(lambda x: None if pd.isna(x) or x == \"\" else str(x))\n", "\n", "# --- Filter allergies to valid patient_ids ---\n", "allergies_df = allergies_df[allergies_df[\"patient_id\"].isin(patients_df[\"patient_id\"])].copy()\n", "\n", "# Confirm how many rows remain\n", "print(f\"Patients: {len(patients_df)} | Allergies (filtered): {len(allergies_df)}\")\n", "# Run it\n", "copy_dataframe(patients_df, \"patients\")\n", "copy_dataframe(allergies_df, \"allergies\")\n"]}, {"cell_type": "markdown", "id": "2dfffc16-db86-46aa-9aa7-96c3edde54f4", "metadata": {}, "source": ["# 7. Join data into patient ‚Äúcontext‚Äù paragraphs"]}, {"cell_type": "code", "execution_count": 7, "id": "14953bbc-90be-4c46-a9ab-a1f4eb7ef9a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["‚úÖ Created 1000 patient context records with labeled facts\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/tmp/ipykernel_340199/1374959709.py:41: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n", "  df_context = pd.read_sql(query, conn)\n"]}, {"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>patient_id</th>\n", "      <th>context_text</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>004b6780-7827-4030-a9e7-b7c9f44b2fab</td>\n", "      <td>Patient: John Weaver. Gender: female; Age: 54....</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>00840f4a-78ed-4b56-99d4-b3b375ccc277</td>\n", "      <td>Patient: Andrew Lopez. Gender: other; Age: 115...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                             patient_id  \\\n", "0  004b6780-7827-4030-a9e7-b7c9f44b2fab   \n", "1  00840f4a-78ed-4b56-99d4-b3b375ccc277   \n", "\n", "                                        context_text  \n", "0  Patient: John Weaver. Gender: female; Age: 54....  \n", "1  Patient: Andrew Lopez. Gender: other; Age: 115...  "]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["# --- Step 7 (replace completely) ---\n", "query = \"\"\"\n", "WITH agg AS (\n", "  SELECT\n", "    p.patient_id,\n", "    p.first_name, p.last_name, p.gender, p.age, p.race, p.ethnicity,\n", "    p.sdoh_risk_score,\n", "    CASE\n", "      WHEN p.sdoh_risk_score >= 0.70 THEN 'HIGH'\n", "      WHEN p.sdoh_risk_score >= 0.40 THEN 'MEDIUM'\n", "      ELSE 'LOW'\n", "    END AS sdoh_bucket,\n", "    COALESCE(p.insurance, 'Unknown') AS insurance,\n", "    COALESCE(p.smoking_status, 'Unknown') AS smoking_status,\n", "    COALESCE(p.deceased, FALSE) AS deceased,\n", "    NULLIF(p.death_date, '') AS death_date_raw,\n", "    COUNT(a.allergy_id) AS allergy_count,\n", "    MAX(CASE WHEN LOWER(COALESCE(a.severity,'')) IN ('severe','high','life-threatening') THEN 1 ELSE 0 END) AS any_severe_allergy,\n", "    STRING_AGG(a.substance || ' (' || COALESCE(a.severity,'unknown') || ')', '; ' ORDER BY a.substance) AS allergy_list\n", "  FROM patients p\n", "  LEFT JOIN allergies a ON p.patient_id = a.patient_id\n", "  GROUP BY p.patient_id, p.first_name, p.last_name, p.gender, p.age, p.race, p.ethnicity,\n", "           p.sdoh_risk_score, insurance, smoking_status, deceased, p.death_date\n", ")\n", "SELECT\n", "  patient_id,\n", "  CONCAT(\n", "    'Patient: ', first_name, ' ', last_name, '. ',\n", "    'Gender: ', COALESCE(gender,'unknown'), '; Age: ', COALESCE(age::text,'unknown'), '. ',\n", "    'Race: ', COALESCE(race,'unspecified'), '; Ethnicity: ', COALESCE(ethnicity,'unspecified'), '. ',\n", "    'SDOH: ', sdoh_bucket, ' (', COALESCE(sdoh_risk_score::text,'n/a'), '). ',\n", "    'Insurance: ', insurance, '; Smoking: ', smoking_status, '. ',\n", "    'Deceased: ', CASE WHEN deceased THEN 'yes' ELSE 'no' END,\n", "    CASE WHEN deceased AND death_date_raw IS NOT NULL THEN CONCAT(' (death_date=', death_date_raw, ')') ELSE '' END, '. ',\n", "    'Allergies: ', COALESCE(allergy_list,'none'), '. ',\n", "    'Allergy_count: ', allergy_count::text, '; Any_severe_allergy: ', any_severe_allergy::text, '.'\n", "  ) AS context_text\n", "FROM agg;\n", "\"\"\"\n", "\n", "df_context = pd.read_sql(query, conn)\n", "print(f\"‚úÖ Created {len(df_context)} patient context records with labeled facts\")\n", "df_context.head(2)\n"]}, {"cell_type": "markdown", "id": "80907c20", "metadata": {}, "source": ["# 8a. Configure embedding backend (Ollama or LLM API)\n", "\n", "Set `EMBEDDING_BACKEND` to `\"ollama\"` for the local Ollama workflow or `\"llm_api\"` to call an OpenAPI-compatible embedding service. When using the hosted path, export `OPENAI_API_KEY` (and optionally `OPENAI_BASE_URL`, `OPENAI_EMBED_MODEL`, `OPENAI_ORG`) before running the cell. Use `EMBEDDING_DIM` if the remote model emits vectors larger/smaller than the pgvector column, and set `SKIP_EMBEDDING_SMOKETEST=1` to bypass the quick connectivity check.\n"]}, {"cell_type": "code", "execution_count": 11, "id": "4963a5cf", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["üì¶ Embedding backend: ollama\n", " - Ollama model: phi4-mini\n"]}], "source": ["EMBEDDING_BACKEND = os.getenv('EMBEDDING_BACKEND', 'ollama').strip().lower()\n", "if EMBEDDING_BACKEND not in {'ollama', 'llm_api'}:\n", "    raise ValueError(f'Unsupported EMBEDDING_BACKEND: {EMBEDDING_BACKEND}')\n", "\n", "OLLAMA_EMBED_MODEL = os.getenv('OLLAMA_EMBED_MODEL', 'phi4-mini')\n", "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n", "OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1').rstrip('/')\n", "OPENAI_EMBED_MODEL = os.getenv('OPENAI_EMBED_MODEL', 'text-embedding-3-small')\n", "OPENAI_ORG = os.getenv('OPENAI_ORG')\n", "EMBEDDING_TIMEOUT = int(os.getenv('EMBEDDING_TIMEOUT', '30'))\n", "EMBEDDING_DIM_OVERRIDE = os.getenv('EMBEDDING_DIM')\n", "EMBEDDING_DIM_OVERRIDE = int(EMBEDDING_DIM_OVERRIDE) if EMBEDDING_DIM_OVERRIDE else None\n", "SKIP_EMBEDDING_SMOKETEST = os.getenv('SKIP_EMBEDDING_SMOKETEST', '0') == '1'\n", "\n", "if EMBEDDING_BACKEND == 'llm_api' and not OPENAI_API_KEY:\n", "    raise RuntimeError('Set OPENAI_API_KEY before selecting the llm_api backend.')\n", "\n", "print(f'üì¶ Embedding backend: {EMBEDDING_BACKEND}')\n", "if EMBEDDING_BACKEND == 'ollama':\n", "    print(f' - Ollama model: {OLLAMA_EMBED_MODEL}')\n", "else:\n", "    print(f' - API base URL: {OPENAI_BASE_URL}/embeddings')\n", "    print(f' - Remote model: {OPENAI_EMBED_MODEL}')\n", "    if EMBEDDING_DIM_OVERRIDE:\n", "        print(f' - Expected vector length override: {EMBEDDING_DIM_OVERRIDE}')\n", "if SKIP_EMBEDDING_SMOKETEST:\n", "    print('‚è≠Ô∏è Skipping embedding smoke test will defer dimension discovery to Step 9.')\n"]}, {"cell_type": "markdown", "id": "42df8cf5-8efa-4796-9a51-002f27a0c500", "metadata": {}, "source": ["# 8b. Embedding helper functions\n", "\n", "Dispatch requests to the selected backend, normalize outputs, and guard against vector length mismatches before storing embeddings in Postgres.\n"]}, {"cell_type": "code", "execution_count": 10, "id": "c39a2538-b88e-465b-9bdf-39ae7e024bc5", "metadata": {}, "outputs": [{"ename": "ConnectionError", "evalue": "HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78240012dd90>: Failed to establish a new connection: [Errno 111] Connection refused'))", "output_type": "error", "traceback": ["\u001b[31m---------------------------------------------------------------------------\u001b[39m", "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n", "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused", "\nThe above exception was the direct cause of the following exception:\n", "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connection.py:494\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28mself\u001b[39m.putheader(header, value)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connection.py:325\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n\u001b[32m    327\u001b[39m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connection.py:213\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    214\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    217\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n", "\u001b[31mNewConnectionError\u001b[39m: <urllib3.connection.HTTPConnection object at 0x78240012dd90>: Failed to establish a new connection: [Errno 111] Connection refused", "\nThe above exception was the direct cause of the following exception:\n", "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n", "\u001b[31mMaxRetryError\u001b[39m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78240012dd90>: Failed to establish a new connection: [Errno 111] Connection refused'))", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vector\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SKIP_EMBEDDING_SMOKETEST:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     smoke_vec = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest patient embedding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_BACKEND\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m embedding length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(smoke_vec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(text, model)\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mText to embed must be non-empty.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m EMBEDDING_BACKEND == \u001b[33m'\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     vector = \u001b[43m_embed_with_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mOLLAMA_EMBED_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     vector = _embed_with_llm_api(text, model \u001b[38;5;129;01mor\u001b[39;00m OPENAI_EMBED_MODEL)\n", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m_embed_with_ollama\u001b[39m\u001b[34m(text, model)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_with_ollama\u001b[39m(text, model):\n\u001b[32m      5\u001b[39m     payload = {\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: model, \u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m: text}\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp://localhost:11434/api/embeddings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEMBEDDING_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     response.raise_for_status()\n\u001b[32m      8\u001b[39m     data = response.json()\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n", "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/adapters.py:677\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n", "\u001b[31mConnectionError\u001b[39m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78240012dd90>: Failed to establish a new connection: [Errno 111] Connection refused'))"]}], "source": ["EXPECTED_EMBEDDING_DIM = EMBEDDING_DIM_OVERRIDE\n", "\n", "\n", "def _embed_with_ollama(text, model):\n", "    payload = {'model': model, 'prompt': text}\n", "    response = requests.post('http://localhost:11434/api/embeddings', json=payload, timeout=EMBEDDING_TIMEOUT)\n", "    response.raise_for_status()\n", "    data = response.json()\n", "    if 'embedding' not in data:\n", "        raise ValueError(f'Unexpected Ollama response: {data}')\n", "    return data['embedding']\n", "\n", "\n", "def _embed_with_llm_api(text, model):\n", "    headers = {'Authorization': f'Bearer {OPENAI_API_KEY}'}\n", "    if OPENAI_ORG:\n", "        headers['OpenAI-Organization'] = OPENAI_ORG\n", "    payload = {'model': model, 'input': text}\n", "    response = requests.post(f'{OPENAI_BASE_URL}/embeddings', headers=headers, json=payload, timeout=EMBEDDING_TIMEOUT)\n", "    response.raise_for_status()\n", "    body = response.json()\n", "    try:\n", "        return body['data'][0]['embedding']\n", "    except (KeyError, IndexError) as exc:\n", "        raise ValueError(f'Unexpected LLM API response: {body}') from exc\n", "\n", "\n", "def get_embedding(text, *, model=None):\n", "    global EXPECTED_EMBEDDING_DIM\n", "    if not text:\n", "        raise ValueError('Text to embed must be non-empty.')\n", "    if EMBEDDING_BACKEND == 'ollama':\n", "        vector = _embed_with_ollama(text, model or OLLAMA_EMBED_MODEL)\n", "    else:\n", "        vector = _embed_with_llm_api(text, model or OPENAI_EMBED_MODEL)\n", "    length = len(vector)\n", "    if EXPECTED_EMBEDDING_DIM is None:\n", "        EXPECTED_EMBEDDING_DIM = length\n", "    elif length != EXPECTED_EMBEDDING_DIM:\n", "        raise ValueError(f'Embedding length mismatch: expected {EXPECTED_EMBEDDING_DIM}, got {length}')\n", "    return vector\n", "\n", "\n", "if not SKIP_EMBEDDING_SMOKETEST:\n", "    smoke_vec = get_embedding('test patient embedding')\n", "    print(f'‚úÖ {EMBEDDING_BACKEND} embedding length: {len(smoke_vec)}')\n", "else:\n", "    print('‚ÑπÔ∏è Smoke test deferred; the first embedding call will determine vector length.')\n"]}, {"cell_type": "markdown", "id": "946a191e-9457-4dc6-a84a-6ec02e8798ec", "metadata": {}, "source": ["# 9. Create and fill patient_embeddings\n", "\n", "The table uses the discovered embedding dimension (from the smoke test or the first context embedding) so it works with either backend.\n"]}, {"cell_type": "code", "execution_count": null, "id": "75e0d78c-7b09-4bda-9cf8-ea995727dd9b", "metadata": {}, "outputs": [], "source": ["if df_context.empty:\n", "    raise ValueError('df_context is empty; run Step 7 before generating embeddings.')\n", "\n", "cached_first_embedding = None\n", "if EXPECTED_EMBEDDING_DIM is None:\n", "    cached_first_embedding = get_embedding(df_context.iloc[0].context_text)\n", "    vector_length = len(cached_first_embedding)\n", "else:\n", "    vector_length = EXPECTED_EMBEDDING_DIM\n", "\n", "cur.execute('DROP TABLE IF EXISTS patient_embeddings;')\n", "cur.execute(f\"\"\"\n", "CREATE TABLE patient_embeddings (\n", "  patient_id TEXT PRIMARY KEY,\n", "  context_text TEXT,\n", "  embedding VECTOR({vector_length})\n", ");\n", "\"\"\")\n", "conn.commit()\n", "\n", "for idx, row in enumerate(df_context.itertuples(index=False)):\n", "    if idx == 0 and cached_first_embedding is not None:\n", "        emb = cached_first_embedding\n", "    else:\n", "        emb = get_embedding(row.context_text)\n", "    cur.execute(\n", "        'INSERT INTO patient_embeddings (patient_id, context_text, embedding) VALUES (%s, %s, %s)',\n", "        (row.patient_id, row.context_text, emb)\n", "    )\n", "conn.commit()\n", "print(f'‚úÖ Re-embedded {len(df_context)} patients with {vector_length}-d vectors via {EMBEDDING_BACKEND}')\n"]}, {"cell_type": "markdown", "id": "074e0d46-3cff-4680-af71-6149e7ba7765", "metadata": {}, "source": ["# 10. Train UMAP reducer"]}, {"cell_type": "code", "execution_count": null, "id": "3447fb72-ef70-446c-acb7-c4ae39511c6d", "metadata": {}, "outputs": [], "source": ["\n", "import umap\n", "import numpy as np\n", "import ast\n", "\n", "\n", "cur.execute(\"SELECT patient_id, embedding FROM patient_embeddings;\")\n", "rows = cur.fetchall()\n", "\n", "embeddings = np.vstack([\n", "    np.array(ast.literal_eval(r[1]), dtype=np.float32) if isinstance(r[1], str) else np.array(r[1], dtype=np.float32)\n", "    for r in rows\n", "])\n", "patient_ids = [r[0] for r in rows]\n", "\n", "reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, metric=\"cosine\", random_state=42)\n", "embedding_2d = reducer.fit_transform(embeddings)\n", "\n", "print(f\"‚úÖ UMAP trained on {len(patient_ids)} patient embeddings.\")\n"]}, {"cell_type": "markdown", "id": "88b6b8da-a0c2-4776-a279-78102a53f2d1", "metadata": {}, "source": ["# 11. Semantic search query"]}, {"cell_type": "code", "execution_count": null, "id": "edf224d0-8f65-4d23-af87-916d4f22f6c9", "metadata": {}, "outputs": [], "source": ["query_text = 'patients allergic to penicillin with high sdoh risk'\n", "query_emb = get_embedding(query_text)\n", "\n", "sql = \"\"\"\n", "SELECT p.patient_id,\n", "       p.context_text,\n", "       1 - (p.embedding <=> %s::vector) AS similarity\n", "FROM patient_embeddings p\n", "ORDER BY similarity DESC\n", "LIMIT 5;\n", "\"\"\"\n", "\n", "cur.execute(sql, (query_emb,))\n", "results = cur.fetchall()\n", "\n", "pd.DataFrame(results, columns=['patient_id', 'context_text', 'similarity'])\n"]}, {"cell_type": "markdown", "id": "1b34be92-714f-4e31-8b8a-75251b4c68dc", "metadata": {}, "source": ["# 12. Visualization (Matplotlib + Plotly)"]}, {"cell_type": "code", "execution_count": null, "id": "029cad23-2f0a-4c8f-8065-ca728f536393", "metadata": {}, "outputs": [], "source": ["# --- Step 12: Visualize patient embeddings with optional query overlay ---\n", "import matplotlib.pyplot as plt\n", "import plotly.express as px\n", "import numpy as np\n", "import pandas as pd\n", "\n", "plt.figure(figsize=(8,6))\n", "plt.scatter(embedding_2d[:,0], embedding_2d[:,1], s=40, alpha=0.8)\n", "plt.title(\"Patient Embedding Clusters (œÜ4-mini)\")\n", "plt.xlabel(\"UMAP-1\")\n", "plt.ylabel(\"UMAP-2\")\n", "plt.show()\n", "\n", "# Optional: Interactive hover view\n", "df_plot = pd.DataFrame({\n", "    \"x\": embedding_2d[:,0],\n", "    \"y\": embedding_2d[:,1],\n", "    \"patient_id\": patient_ids,\n", "    \"context_text\": [r[1] for r in rows]\n", "})\n", "\n", "fig = px.scatter(\n", "    df_plot,\n", "    x=\"x\",\n", "    y=\"y\",\n", "    hover_data={\"patient_id\": True, \"context_text\": True},\n", "    title=\"Interactive Semantic Map of Patients\",\n", "    width=800,\n", "    height=600\n", ")\n", "fig.update_traces(marker=dict(size=10, opacity=0.8))\n", "fig.show()\n"]}, {"cell_type": "markdown", "id": "65658a80-27a9-45a7-88aa-6c74f20df8da", "metadata": {}, "source": ["# 12. Add NLP Semantic Search for Patients"]}, {"cell_type": "code", "execution_count": null, "id": "6907f219-9182-46ac-b8bf-8e9a4f930050", "metadata": {}, "outputs": [], "source": ["import json\n", "import numpy as np\n", "import pandas as pd\n", "from tabulate import tabulate\n", "from sklearn.metrics.pairwise import cosine_similarity\n", "\n", "\n", "def embed_query_vector(text):\n", "    return np.array(get_embedding(text), dtype=np.float32)\n", "\n", "\n", "def semantic_search_fused(query_text, top_k=5):\n", "    # Parse rule-based filters\n", "    filters = parse_filters(query_text)\n", "    candidate_ids = candidate_ids_from_filters(filters)\n", "\n", "    if not candidate_ids:\n", "        cur.execute('SELECT patient_id, embedding FROM patient_embeddings;')\n", "    else:\n", "        cur.execute(\n", "            \"\"\"\n", "            SELECT patient_id, embedding\n", "            FROM patient_embeddings\n", "            WHERE patient_id = ANY(%s);\n", "        \"\"\",\n", "            (candidate_ids,)\n", "        )\n", "    rows = cur.fetchall()\n", "\n", "    if not rows:\n", "        print('No candidates match the prefilters.')\n", "        return None, pd.DataFrame()\n", "\n", "    q_emb = embed_query_vector(query_text).reshape(1, -1)\n", "\n", "    ids, mats = [], []\n", "    for pid, emb in rows:\n", "        if isinstance(emb, str):\n", "            emb = json.loads(emb)\n", "        mats.append(np.array(emb, dtype=np.float32))\n", "        ids.append(pid)\n", "    E = np.vstack(mats)\n", "\n", "    sims = cosine_similarity(q_emb, E)[0]\n", "    order = np.argsort(-sims)[:top_k]\n", "    top = [(ids[i], float(sims[i])) for i in order]\n", "\n", "    results = []\n", "\n", "    for pid, score in top:\n", "        cur.execute(\n", "            \"\"\"\n", "            SELECT first_name, last_name, gender, age, race, ethnicity,\n", "                   sdoh_risk_score, insurance, smoking_status, deceased, death_date\n", "            FROM patients WHERE patient_id = %s;\n", "        \"\"\",\n", "            (pid,)\n", "        )\n", "        p = cur.fetchone()\n", "\n", "        cur.execute(\n", "            \"\"\"\n", "            SELECT substance, severity, reaction\n", "            FROM allergies WHERE patient_id = %s\n", "            ORDER BY severity DESC NULLS LAST, substance\n", "            LIMIT 3;\n", "        \"\"\",\n", "            (pid,)\n", "        )\n", "        alls = cur.fetchall()\n", "        allergy_summary = '; '.join([f\"{a[0]} ({a[1] or 'unknown'})\" for a in alls]) if alls else 'None'\n", "\n", "        sdoh_bucket = 'HIGH' if (p[6] and p[6] >= 0.70) else ('MEDIUM' if (p[6] and p[6] >= 0.40) else 'LOW')\n", "\n", "        # --- EXPLANATION SECTION ---\n", "        explanations = []\n", "        if filters['gender']:\n", "            explanations.append(f\"gender={p[2]} {'‚úÖ' if p[2] and p[2].lower()==filters['gender'] else '‚ùå'}\")\n", "        if filters['sdoh_bucket']:\n", "            explanations.append(f\"SDOH={sdoh_bucket} {'‚úÖ' if sdoh_bucket==filters['sdoh_bucket'] else '‚ùå'}\")\n", "        if filters['deceased'] is not None:\n", "            explanations.append(f\"deceased={p[9]} {'‚úÖ' if bool(p[9])==filters['deceased'] else '‚ùå'}\")\n", "        if filters['recent_days']:\n", "            explanations.append(f\"recent_check={filters['recent_days']}d window\")\n", "        explanation = '; '.join(explanations) if explanations else 'No explicit filter matches'\n", "\n", "        results.append({\n", "            'Patient ID': pid,\n", "            'Name': f\"{p[0]} {p[1]}\",\n", "            'Gender': p[2],\n", "            'Age': p[3],\n", "            'SDOH': f\"{sdoh_bucket} ({p[6]})\",\n", "            'Insurance': p[7],\n", "            'Smoking': p[8],\n", "            'Deceased': p[9],\n", "            'Death Date': p[10],\n", "            'Allergies (sample)': allergy_summary,\n", "            'Similarity': round(score, 3),\n", "            'Filter Match Summary': explanation\n", "        })\n", "\n", "    df = pd.DataFrame(results)\n", "\n", "    print(f\"\\nüîé Query: {query_text}\")\n", "    print(tabulate(df, headers='keys', tablefmt='fancy_grid', showindex=False))\n", "\n", "    return q_emb, df\n"]}, {"cell_type": "markdown", "id": "737e9142-18af-49ac-8171-2701022ba9ea", "metadata": {}, "source": ["# 13. Filtering + fused semantic search"]}, {"cell_type": "code", "execution_count": null, "id": "974ddb12-527f-46aa-b517-d20e83bca2d9", "metadata": {}, "outputs": [], "source": ["import re\n", "from datetime import datetime, timedelta\n", "\n", "def parse_filters(query_text):\n", "    q = query_text.lower()\n", "\n", "    filters = {\n", "        \"gender\": None,\n", "        \"sdoh_bucket\": None,\n", "        \"deceased\": None,\n", "        \"recent_days\": None\n", "    }\n", "\n", "    if \"female\" in q: filters[\"gender\"] = \"female\"\n", "    if \"male\" in q and \"female\" not in q: filters[\"gender\"] = \"male\"\n", "\n", "    if \"high sdoh\" in q or \"high social risk\" in q:   filters[\"sdoh_bucket\"] = \"HIGH\"\n", "    elif \"low sdoh\" in q or \"low social risk\" in q:   filters[\"sdoh_bucket\"] = \"LOW\"\n", "    elif \"medium sdoh\" in q or \"medium social risk\" in q: filters[\"sdoh_bucket\"] = \"MEDIUM\"\n", "\n", "    if \"deceased\" in q or \"death\" in q:\n", "        filters[\"deceased\"] = True\n", "        if \"recent\" in q or \"recently\" in q:\n", "            filters[\"recent_days\"] = 180  # tweak as needed\n", "\n", "    return filters\n", "def candidate_ids_from_filters(filters):\n", "    clauses, params = [], []\n", "\n", "    if filters[\"gender\"]:\n", "        clauses.append(\"LOWER(gender) = %s\")\n", "        params.append(filters[\"gender\"])\n", "\n", "    if filters[\"sdoh_bucket\"]:\n", "        clauses.append(\"\"\"\n", "        CASE\n", "          WHEN sdoh_risk_score >= 0.70 THEN 'HIGH'\n", "          WHEN sdoh_risk_score >= 0.40 THEN 'MEDIUM'\n", "          ELSE 'LOW'\n", "        END = %s\n", "        \"\"\")\n", "        params.append(filters[\"sdoh_bucket\"])\n", "\n", "    if filters[\"deceased\"] is True and filters[\"recent_days\"]:\n", "        days = int(filters[\"recent_days\"])\n", "        clauses.append(f\"\"\"\n", "          COALESCE(deceased, FALSE) = TRUE\n", "          AND NULLIF(death_date,'') IS NOT NULL\n", "          AND death_date ~ '^[0-9]{{4}}-[0-9]{{2}}-[0-9]{{2}}$'\n", "          AND TO_DATE(death_date, 'YYYY-MM-DD') >= (CURRENT_DATE - INTERVAL '{days} days')\n", "        \"\"\")\n", "    \n", "    elif filters[\"deceased\"] is True:\n", "        clauses.append(\"COALESCE(deceased, FALSE) = TRUE\")\n", "\n", "    where_sql = (\"WHERE \" + \" AND \".join(clauses)) if clauses else \"\"\n", "    sql = f\"SELECT patient_id FROM patients {where_sql};\"\n", "    cur.execute(sql, params)\n", "    return [r[0] for r in cur.fetchall()]\n"]}, {"cell_type": "markdown", "id": "449cb275-e536-4938-8186-60da88ea0469", "metadata": {}, "source": ["# Run queries"]}, {"cell_type": "code", "execution_count": null, "id": "67e36b93-81ed-47bc-b864-87dd9ac610f2", "metadata": {}, "outputs": [], "source": ["q_emb, df_results = semantic_search_fused(\"patients experiencing headaches\", top_k=5)\n", "\n", "semantic_search_fused(\"recently deceased patient with respiratory reaction and low income\", top_k=5)\n", "\n", "semantic_search_fused(\"female patient deceased recently with severe drug allergy\", top_k=5)\n"]}, {"cell_type": "code", "execution_count": null, "id": "d62359aa-fa31-477f-8615-d1eb3d755019", "metadata": {}, "outputs": [], "source": ["# --- Step 13: Project query embedding into same UMAP ---\n", "q_emb = np.array(q_emb, dtype=np.float32).reshape(1, -1)\n", "q_emb_2d = reducer.transform(q_emb)\n", "\n", "plt.figure(figsize=(8,6))\n", "plt.scatter(embedding_2d[:,0], embedding_2d[:,1], alpha=0.3, label='Patients')\n", "plt.scatter(q_emb_2d[:,0], q_emb_2d[:,1], color='red', s=120, label='Query')\n", "plt.legend()\n", "plt.title(\"Query Position in Patient Semantic Space\")\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "12833126-1b7b-4dcd-adc6-da6dbbb00320", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.12"}}, "nbformat": 4, "nbformat_minor": 5}